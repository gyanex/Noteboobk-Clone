import { SafetySetting, HttpOptions, Session, LiveServerMessage, GenerateContentConfig, GoogleGenAI, GoogleGenAIOptions, GenerateContentResponse, Blob, FunctionDeclaration, Modality, Content, Part } from '@google/genai';
export { Modality } from '@google/genai';
import { MessageType, LiveLLM, LiveConnectConfig, LiveLLMSession, MessageSender, BaseTool, ToolCallLLM, LLMMetadata, LLMChatParamsStreaming, ToolCallLLMMessageOptions, ChatResponseChunk, LLMChatParamsNonStreaming, ChatResponse, LLMCompletionParamsStreaming, CompletionResponse, LLMCompletionParamsNonStreaming, ToolCall, MessageContentDetail } from '@llamaindex/core/llms';
import { ModalityType } from '@llamaindex/core/schema';
import { BaseEmbedding, BaseEmbeddingOptions } from '@llamaindex/core/embeddings';

declare enum GEMINI_MODEL {
    GEMINI_PRO = "gemini-pro",
    GEMINI_PRO_VISION = "gemini-pro-vision",
    GEMINI_PRO_LATEST = "gemini-1.5-pro-latest",
    GEMINI_PRO_FLASH_LATEST = "gemini-1.5-flash-latest",
    GEMINI_PRO_1_5_PRO_PREVIEW = "gemini-1.5-pro-preview-0514",
    GEMINI_PRO_1_5_FLASH_PREVIEW = "gemini-1.5-flash-preview-0514",
    GEMINI_PRO_1_5 = "gemini-1.5-pro-001",
    GEMINI_PRO_1_5_FLASH = "gemini-1.5-flash-001",
    GEMINI_PRO_1_5_LATEST = "gemini-1.5-pro-002",
    GEMINI_PRO_1_5_FLASH_LATEST = "gemini-1.5-flash-002",
    GEMINI_2_0_FLASH_EXPERIMENTAL = "gemini-2.0-flash-exp",
    GEMINI_2_0_FLASH = "gemini-2.0-flash-001",
    GEMINI_2_0_FLASH_LITE = "gemini-2.0-flash-lite-001",
    GEMINI_2_0_FLASH_LITE_PREVIEW = "gemini-2.0-flash-lite-preview-02-05",
    GEMINI_2_0_FLASH_THINKING_EXP = "gemini-2.0-flash-thinking-exp-01-21",
    GEMINI_2_0_PRO_EXPERIMENTAL = "gemini-2.0-pro-exp-02-05",
    GEMINI_2_0_FLASH_LIVE = "gemini-2.0-flash-live-001",
    GEMINI_2_5_PRO_PREVIEW = "gemini-2.5-pro-preview-03-25",
    GEMINI_2_5_PRO_PREVIEW_LATEST = "gemini-2.5-pro-preview-06-05",
    GEMINI_2_5_FLASH_PREVIEW = "gemini-2.5-flash-preview-05-20",
    GEMINI_2_5_PRO_LATEST = "gemini-2.5-pro",
    GEMINI_2_5_FLASH_LATEST = "gemini-2.5-flash",
    GEMINI_2_5_FLASH_LITE = "gemini-2.5-flash-lite"
}
declare const GEMINI_MODEL_INFO_MAP: Record<GEMINI_MODEL, {
    contextWindow: number;
}>;
declare const SUPPORT_TOOL_CALL_MODELS: GEMINI_MODEL[];
declare const DEFAULT_GEMINI_PARAMS: {
    model: GEMINI_MODEL;
    temperature: number;
    topP: number;
    maxTokens: undefined;
};
/**
 * Safety settings to disable external filters
 * Documentation: https://ai.google.dev/gemini-api/docs/safety-settings
 */
declare const DEFAULT_SAFETY_SETTINGS: SafetySetting[];
declare enum GEMINI_MESSAGE_ROLE {
    USER = "user",
    MODEL = "model"
}
declare const ROLES_TO_GEMINI: Record<MessageType, GEMINI_MESSAGE_ROLE>;
declare const ROLES_FROM_GEMINI: Record<GEMINI_MESSAGE_ROLE, MessageType>;

type GeminiVoiceName = "Puck" | "Charon" | "Fenrir" | "Aoede" | "Leda" | "Kore" | "Orus" | "Zephyr";
interface GeminiLiveConfig {
    apiKey?: string | undefined;
    voiceName?: GeminiVoiceName | undefined;
    model?: GEMINI_MODEL | undefined;
    httpOptions?: HttpOptions | undefined;
}
declare class GeminiLiveSession extends LiveLLMSession {
    session: Session | undefined;
    closed: boolean;
    constructor();
    get messageSender(): MessageSender;
    private isInterruptedEvent;
    private isGenerationCompleteEvent;
    private isTurnCompleteEvent;
    private isTextEvent;
    private isAudioEvent;
    private isToolCallEvent;
    private isSetupCompleteEvent;
    private handleToolCallEvent;
    handleLiveEvents(event: LiveServerMessage, toolCalls: BaseTool[]): void;
    private executeToolCall;
    private storeToolCallResponse;
    private executeToolCallsAndStoreResponses;
    private sendToolCallResponses;
    disconnect(): Promise<void>;
}
declare class GeminiLive extends LiveLLM {
    private apiKey;
    private client;
    voiceName?: GeminiVoiceName | undefined;
    model: GEMINI_MODEL;
    httpOptions?: HttpOptions | undefined;
    constructor(init?: GeminiLiveConfig);
    getEphemeralKey(): Promise<string>;
    connect(config?: LiveConnectConfig): Promise<GeminiLiveSession>;
}

type GeminiAdditionalChatOptions = object;
type GeminiChatParamsStreaming = LLMChatParamsStreaming<GeminiAdditionalChatOptions, ToolCallLLMMessageOptions>;
type GeminiChatStreamResponse = AsyncIterable<ChatResponseChunk<ToolCallLLMMessageOptions>>;
type GeminiChatParamsNonStreaming = LLMChatParamsNonStreaming<GeminiAdditionalChatOptions, ToolCallLLMMessageOptions>;
type GeminiChatNonStreamResponse = ChatResponse<ToolCallLLMMessageOptions>;
type GeminiConfig = {
    model?: GEMINI_MODEL;
    temperature?: number;
    topP?: number;
    maxTokens?: number;
    safetySettings?: SafetySetting[];
    voiceName?: GeminiVoiceName;
    apiKey?: string;
    httpOptions?: HttpOptions;
    vertex?: {
        project?: string;
        location?: string;
    };
};
/**
 * ToolCallLLM for Gemini
 */
declare class Gemini extends ToolCallLLM<GeminiAdditionalChatOptions> {
    private client;
    model: GEMINI_MODEL;
    temperature: number;
    topP: number;
    maxTokens?: number | undefined;
    safetySettings: SafetySetting[];
    apiKey?: string | undefined;
    private _live;
    voiceName?: GeminiVoiceName | undefined;
    httpOptions?: HttpOptions | undefined;
    constructor(init?: GeminiConfig);
    get supportToolCall(): boolean;
    get live(): GeminiLive;
    get metadata(): LLMMetadata & {
        safetySettings: SafetySetting[];
    };
    get generationConfig(): GenerateContentConfig;
    chat(params: GeminiChatParamsStreaming): Promise<GeminiChatStreamResponse>;
    chat(params: GeminiChatParamsNonStreaming): Promise<GeminiChatNonStreamResponse>;
    complete(params: LLMCompletionParamsStreaming): Promise<AsyncIterable<CompletionResponse>>;
    complete(params: LLMCompletionParamsNonStreaming): Promise<CompletionResponse>;
    private nonStreamChat;
    private streamChat;
    private streamGenerate;
    private nonStreamGenerate;
    private prepareChatConfig;
    /**
     * Prepare chat history and last message for chatting
     * @param messages - array of LlamaIndex ChatMessage
     * @returns chat history and last message
     */
    private prepareChatContext;
    private messageToGeminiParts;
    private messageContentToGeminiParts;
}
/**
 * Convenience function to create a new Gemini instance.
 * @param init - Optional initialization parameters for the Gemini instance.
 * @returns A new Gemini instance.
 */
declare const gemini: (init?: ConstructorParameters<typeof Gemini>[0]) => Gemini;

type GoogleAdditionalChatOptions = {
    config: GenerateContentConfig;
};
type GoogleChatStreamResponse = AsyncIterable<ChatResponseChunk<ToolCallLLMMessageOptions & {
    data?: Blob[];
}>>;
type GoogleChatParamsStreaming = LLMChatParamsStreaming<GoogleAdditionalChatOptions, ToolCallLLMMessageOptions>;
type GoogleChatParamsNonStreaming = LLMChatParamsNonStreaming<GoogleAdditionalChatOptions, ToolCallLLMMessageOptions>;
type GoogleChatNonStreamResponse = ChatResponse<ToolCallLLMMessageOptions>;
declare const getGoogleStudioInlineData: (response: GenerateContentResponse) => Blob[];
type GoogleModelParams = {
    model: GEMINI_MODEL;
    temperature?: number;
    topP?: number;
    maxTokens?: number;
};
type GoogleParams = GoogleGenAIOptions & GoogleModelParams;
declare class GoogleStudio extends ToolCallLLM<GoogleAdditionalChatOptions> {
    client: GoogleGenAI;
    model: GEMINI_MODEL;
    temperature: number;
    topP: number;
    maxTokens?: number | undefined;
    topK?: number;
    constructor({ temperature, topP, maxTokens, model, ...params }: GoogleParams);
    get supportToolCall(): boolean;
    get metadata(): LLMMetadata;
    getToolCallsFromResponse(response: GenerateContentResponse): ToolCall[];
    protected nonStreamChat(params: GoogleChatParamsNonStreaming): Promise<GoogleChatNonStreamResponse>;
    reduceStream(stream: AsyncGenerator<GenerateContentResponse>): AsyncIterable<ChatResponseChunk>;
    protected streamChat(params: GoogleChatParamsStreaming): GoogleChatStreamResponse;
    chat(params: GoogleChatParamsStreaming): Promise<GoogleChatStreamResponse>;
    chat(params: GoogleChatParamsNonStreaming): Promise<GoogleChatNonStreamResponse>;
    complete(params: LLMCompletionParamsStreaming): Promise<AsyncIterable<CompletionResponse>>;
    complete(params: LLMCompletionParamsNonStreaming): Promise<CompletionResponse>;
}

declare const mapBaseToolToGeminiFunctionDeclaration: (tool: BaseTool) => FunctionDeclaration;
/**
 * Maps a BaseTool to a Gemini Live Function Declaration format
 * Used for converting LlamaIndex tools to be compatible with Gemini's live API function calling
 *
 * @param tool - The BaseTool to convert
 * @returns A LiveFunctionDeclaration object that can be used with Gemini's live API
 */
declare function mapBaseToolToGeminiLiveFunctionDeclaration(tool: BaseTool): FunctionDeclaration;
declare function mapResponseModalityToGeminiLiveResponseModality(responseModality: ModalityType): Modality;
declare function mergeNeighboringSameRoleMessages(messages: Content[]): Content[];
/**
 * Converts a MessageContentDetail object into a Google Gemini Part object.
 *
 * This function handles different content types appropriately for the Gemini API:
 * - Text content: Directly converts to Gemini text part
 * - Image URLs: Extracts MIME type and creates part from URI
 * - File/media content: Uploads to Google servers first, then creates part from uploaded URI
 *
 * @param content - The content to be converted (text, image URL, or base64 file data)
 * @param client - Google GenAI client
 *
 * @returns Promise that resolves to a Gemini-compatible Part object
 *
 * @throws {Error} When MIME type cannot be extracted from image URL
 * @throws {Error} When file upload fails
 * @throws {Error} When upload succeeds but URI or MIME type is missing from result
 */
declare function messageContentDetailToGeminiPart(content: MessageContentDetail, client: GoogleGenAI): Promise<Part>;

declare enum GEMINI_EMBEDDING_MODEL {
    EMBEDDING_001 = "embedding-001",
    TEXT_EMBEDDING_004 = "text-embedding-004"
}
declare const DEFAULT_EMBED_BATCH_SIZE = 100;
/**
 * Configuration options for GeminiEmbedding.
 */
type GeminiEmbeddingOptions = {
    model?: GEMINI_EMBEDDING_MODEL;
    embedBatchSize?: number;
} & GoogleGenAIOptions;
/**
 * GeminiEmbedding is an alias for Gemini that implements the BaseEmbedding interface.
 */
declare class GeminiEmbedding extends BaseEmbedding {
    model: GEMINI_EMBEDDING_MODEL;
    ai: GoogleGenAI;
    embedBatchSize: number;
    constructor(opts?: GeminiEmbeddingOptions);
    getTextEmbeddings: (texts: string[]) => Promise<number[][]>;
    getTextEmbeddingsBatch(texts: string[], options?: BaseEmbeddingOptions): Promise<Array<number[]>>;
    getTextEmbedding(text: string): Promise<number[]>;
}

export { DEFAULT_EMBED_BATCH_SIZE, DEFAULT_GEMINI_PARAMS, DEFAULT_SAFETY_SETTINGS, GEMINI_EMBEDDING_MODEL, GEMINI_MESSAGE_ROLE, GEMINI_MODEL, GEMINI_MODEL_INFO_MAP, Gemini, GeminiEmbedding, GeminiLive, GeminiLiveSession, GoogleStudio, ROLES_FROM_GEMINI, ROLES_TO_GEMINI, SUPPORT_TOOL_CALL_MODELS, gemini, getGoogleStudioInlineData, mapBaseToolToGeminiFunctionDeclaration, mapBaseToolToGeminiLiveFunctionDeclaration, mapResponseModalityToGeminiLiveResponseModality, mergeNeighboringSameRoleMessages, messageContentDetailToGeminiPart };
export type { GeminiConfig, GeminiEmbeddingOptions, GeminiLiveConfig, GeminiVoiceName };
